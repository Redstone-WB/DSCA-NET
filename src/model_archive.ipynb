{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSCA-NET\n",
    "\n",
    "- Reference codes : Deep Learning for Healthcare, CS598, UIUC, Spring2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\n",
    "# set seed\n",
    "seed = 100\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"./preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pid.pkl'), 'rb'))\n",
    "x_dem = pickle.load(open(os.path.join(DATA_PATH,'x_dem.pkl'), 'rb'))\n",
    "x_per = pickle.load(open(os.path.join(DATA_PATH,'x_per.pkl'), 'rb'))\n",
    "x_d = pickle.load(open(os.path.join(DATA_PATH,'x_d.pkl'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46520\n",
      "46520\n",
      "46520\n",
      "46520\n"
     ]
    }
   ],
   "source": [
    "print(len(pids))\n",
    "print(len(x_dem))\n",
    "print(len(x_per))\n",
    "print(len(x_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 249\n",
      "DemoGraphic: [0, 1, 1, 3, 0]\n",
      "Examination of first visit: [7.45, 330.0, 41.0, 0]\n",
      "Prescription of first visit: [0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Patient ID:\", pids[0])\n",
    "print(\"DemoGraphic:\", x_dem[0])\n",
    "print(\"Examination of first visit:\", x_per[0][0])\n",
    "print(\"Prescription of first visit:\", x_d[0][0])\n",
    "\n",
    "\n",
    "# for visit in range(len(vids[3])):\n",
    "#     print(f\"\\t{visit}-th visit id:\", vids[3][visit])\n",
    "#     print(f\"\\t{visit}-th visit diagnosis labels:\", seqs[3][visit])\n",
    "#     print(f\"\\t{visit}-th visit diagnosis codes:\", [rtypes[label] for label in seqs[3][visit]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Leaving Only patients who have both of examination and prescriptions\n",
    "dem_no_idx = set()\n",
    "per_no_idx = set()\n",
    "d_no_idx = set()\n",
    "\n",
    "for idx in range(len(x_dem)):\n",
    "    \n",
    "    if len(x_dem[idx]) == 0:\n",
    "        dem_no_idx.add(idx)\n",
    "    \n",
    "    if len(x_per[idx]) == 0:\n",
    "        per_no_idx.add(idx)\n",
    "    \n",
    "    if len(x_d[idx]) == 0:\n",
    "        d_no_idx.add(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_no_idx = per_no_idx | d_no_idx\n",
    "all_idx = set(range(len(x_dem)))\n",
    "alive_idx = all_idx - tot_no_idx\n",
    "\n",
    "alive_idx_list = list(alive_idx)\n",
    "alive_idx_list.sort()\n",
    "\n",
    "\n",
    "x_dem_f = [x_dem[i] for i in alive_idx_list]\n",
    "x_per_f = [x_per[i] for i in alive_idx_list]\n",
    "x_d_f = [x_d[i] for i in alive_idx_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11619\n",
      "11619\n",
      "11619\n"
     ]
    }
   ],
   "source": [
    "print(len(x_dem_f))\n",
    "print(len(x_per_f))\n",
    "print(len(x_d_f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x_dem, x_per, x_d):\n",
    "        \n",
    "        self.x_dem = x_dem\n",
    "        self.x_per = x_per\n",
    "        self.x_d = x_d\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.x_d)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x_dem = self.x_dem[index]\n",
    "        x_per = self.x_per[index]\n",
    "        x_d = self.x_d[index]\n",
    "        return x_dem, x_per, x_d\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(x_dem_f, x_per_f, x_d_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "num_patients = len(x_per_f) # 11619\n",
    "num_visits = [len(patient) for patient in x_per_f] #\n",
    "max_num_visits = max(num_visits)\n",
    "\n",
    "num_codes = 400\n",
    "num_per_codes = 4\n",
    "print(max_num_visits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_d_f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "p = [0] * 4\n",
    "ps = [p] * 7\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0]]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2253"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_per[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. (x_d) For all patients, Number of Visit (len(x_d)) must be same -> Need to be padded\n",
    "## 2. (x_d) For all patients, Number of Prescription codes must be same -> Already Done\n",
    "## 3. (x_per) For all patients, Number of Visit (len(x_per)) must be same -> Need to be padded\n",
    "## 4. Mask -> 0 For padded intputs -> No Need\n",
    "def collate_fn(data):\n",
    "\n",
    "    x_dem_now, x_per_now, x_d_now = zip(*data)\n",
    "\n",
    "    x_dem_now = torch.tensor(x_dem_now[0], dtype=torch.float32)\n",
    "    x_per_now = x_per_now[0]\n",
    "    x_d_now = x_d_now[0]\n",
    "\n",
    "    # print(len(x_per_now))\n",
    "    # print(len(x_d_now))\n",
    "\n",
    "\n",
    "    # x_d_new = torch.zeros((1, max_num_visits, num_codes), dtype=torch.long)\n",
    "    # x_per_new = torch.zeros((1, max_num_visits, num_per_codes), dtype=torch.long)\n",
    "\n",
    "\n",
    "    ## (x_d) \n",
    "    length_valid_visit = len(x_d_now)\n",
    "    # print(\"Valid Visit : {0}\".format(length_valid_visit))\n",
    "    num_pad_visit = max_num_visits - length_valid_visit\n",
    "    pad = [0] * num_codes  # [0, ... , 0] : For 400 zeros\n",
    "    tmp = x_d_now.copy()\n",
    "    tmp.extend([pad] * num_pad_visit)\n",
    "    x_d_now = torch.tensor(tmp.copy(), dtype=torch.float32)\n",
    "\n",
    "    # (x_per)\n",
    "    length_valid = len(x_per_now)\n",
    "    num_pad_visit = max_num_visits - length_valid\n",
    "    pad = [0] * num_per_codes\n",
    "    tmp2 = x_per_now.copy()\n",
    "    tmp2.extend([pad] * num_pad_visit)\n",
    "    x_per_now = torch.tensor(tmp2.copy(), dtype=torch.float32)\n",
    "    \n",
    "    return x_dem_now, x_per_now, x_d_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x_dem_new, x_per_new, x_d_new = next(loader_iter)\n",
    "# print(x[0])\n",
    "# print(x_d_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([76, 4])\n",
      "torch.Size([76, 400])\n"
     ]
    }
   ],
   "source": [
    "# x_per, x_d : num patients, max num visits, num codes\n",
    "# x_dem : num_patients, num demo codes\n",
    "\n",
    "print(x_dem_new.size())\n",
    "print(x_per_new.size())  \n",
    "print(x_d_new.size())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 9295\n",
      "Length of val dataset: 2324\n"
     ]
    }
   ],
   "source": [
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    batch_size = 1\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9295\n",
      "2324\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader)) ## divided by 32, no remainder\n",
    "print(len(val_loader)) ## remainder exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([76, 4])\n",
      "torch.Size([76, 400])\n"
     ]
    }
   ],
   "source": [
    "for a, b, c in train_loader:\n",
    "    print(a.size())\n",
    "    print(b.size())\n",
    "    print(c.size())\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross attention\n",
    "def CA(x_dem_per_comb_frac, x_d_new_frac, w_dim=128):\n",
    "    # concatenate x_dem and x_per_new\n",
    "\n",
    "        x_c = x_dem_per_comb_frac.clone()\n",
    "        x_d = x_d_new_frac.clone()\n",
    "\n",
    "        c_linear = nn.Linear(x_c.size()[0], w_dim)\n",
    "        d_linear = nn.Linear(x_d.size()[0], w_dim)\n",
    "        tanh = nn.Tanh()\n",
    "        sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        # print(x_c.size())  # 5 + 128 = 133\n",
    "        # print(x_d.size())  # 128\n",
    "\n",
    "        x_c1 = c_linear(x_c)\n",
    "        x_d1 = d_linear(x_d)\n",
    "\n",
    "        x_c2 = c_linear(x_c)\n",
    "        x_d2 = d_linear(x_d)\n",
    "\n",
    "\n",
    "        s_cd = sigmoid(c_linear(x_c) * tanh(x_c1 + x_d1))\n",
    "        s_dc = sigmoid(d_linear(x_d) * tanh(x_c2 + x_d2))\n",
    "\n",
    "        # weight_c = torch.sum(x_c, dim=0)\n",
    "        # weight_d = torch.sum(x_d, dim=0)\n",
    "        # s_dc = weight_d * x_c\n",
    "        # s_cd = weight_c * x_d\n",
    "\n",
    "\n",
    "        x = torch.concat([torch.mul(s_cd, x_d), torch.mul(s_dc, x_c)])\n",
    "        s = torch.concat([s_cd, s_dc])\n",
    "\n",
    "        # print(x.size())\n",
    "\n",
    "        return x,s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSCARNN(nn.Module):\n",
    "\n",
    "    def __init__(self, h_t_minus_1, ca_t_minus_1, x_t, s_t, num_hidden_dim, num_codes_d, tau):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h_t_minus_1 = h_t_minus_1\n",
    "        self.ca_t_minus_1 = ca_t_minus_1\n",
    "        self.x_t = x_t\n",
    "        self.s_t = s_t\n",
    "        self.num_hidden_dim = num_hidden_dim\n",
    "        self.num_codes_d = num_codes_d\n",
    "        \n",
    "        self.linear = nn.Linear(self.num_hidden_dim * 2, self.num_hidden_dim * 2)\n",
    "        self.linear_dec = nn.Linear(self.num_hidden_dim * 2, self.num_hidden_dim)\n",
    "        self.linear_out = nn.Linear(self.num_hidden_dim *2, self.num_codes_d)\n",
    "    \n",
    "        self.tau = tau\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self):\n",
    "        d_t = self.sigmoid(self.linear(self.x_t))\n",
    "        ca_t = self.s_t + (1 - d_t) * self.ca_t_minus_1\n",
    "\n",
    "        ca_t_tilde = self.tanh(self.linear(ca_t))\n",
    "\n",
    "        h_tilde = self.sigmoid(torch.concat([self.linear_dec(self.x_t), self.linear_dec(self.h_t_minus_1)], dim=0))\n",
    "\n",
    "        h_t = self.tau * ca_t_tilde + (1-self.tau) * self.sigmoid(h_tilde)\n",
    "\n",
    "        y_t_hat = self.sigmoid(self.linear_out(self.relu(self.linear(h_t))))\n",
    "\n",
    "        return h_t, ca_t, y_t_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.zeros((1, 32))\n",
    "# b = torch.zeros((1, 32))\n",
    "# c = torch.concat([a, b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/c4nc3b111l56r38yfxstwq740000gq/T/ipykernel_11756/3365365288.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_hats = torch.tensor(y_t_hat, dtype=torch.float32).unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "class DSCANet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, num_codes_d, tau):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.embedding = nn.Embedding(num_embeddings = num_codes, embedding_dim = 128)\n",
    "        # self.embedding_d = nn.Embedding(num_embeddings = num_codes_d, embedding_dim = 128)\n",
    "        self.num_hidden_dim = 128\n",
    "        self.embedding = nn.Linear(num_codes + 5, self.num_hidden_dim)\n",
    "        self.embedding_d = nn.Linear(num_codes_d, self.num_hidden_dim)\n",
    "\n",
    "        self.num_codes_d = num_codes_d\n",
    "\n",
    "        # self.linear = nn.Linear(self.num_hidden_dim * 2, self.num_hidden_dim * 2)\n",
    "        # self.linear_dec = nn.Linear(self.num_hidden_dim * 2, self.num_hidden_dim)\n",
    "        # self.linear_out = nn.Linear(self.num_hidden_dim *2, num_codes_d)\n",
    "    \n",
    "\n",
    "        # self.rnn = nn.GRU(input_size = 128, hidden_size = 128, batch_first=True)\n",
    "        # self.fc = nn.Linear(in_features = 256, out_features=1)\n",
    "        self.tau = tau\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x_dem_new, x_per_new, x_d_new):\n",
    "        \n",
    "        # batch_size = x_d_new.shape[0] # batch_size = 1\n",
    "        x_dem_new = x_dem_new.squeeze(0) # num_demos\n",
    "        x_per_new = x_per_new.squeeze(0) # num_visit, num_examinations\n",
    "        x_d_new = x_d_new.squeeze(0) #num_visit, num_codes\n",
    "\n",
    "        # print(x_dem_new.size()) # 5\n",
    "        # print(x_per_new.size()) # 76, 4\n",
    "        # print(x_d_new.size()) # 76,400\n",
    "\n",
    "        num_visits = x_per_new.size()[0]\n",
    "\n",
    "        \n",
    "\n",
    "        for v_idx in range(num_visits):\n",
    "            if v_idx == 0:\n",
    "\n",
    "                # if v_idx == 0, initialize h_t and ca_t\n",
    "                h_t_minus_1 = nn.init.kaiming_uniform_(torch.empty(1, self.num_hidden_dim * 2)).squeeze(0)\n",
    "                ca_t_minus_1 = nn.init.kaiming_uniform_(torch.empty(1, self.num_hidden_dim * 2)).squeeze(0)\n",
    "\n",
    "                \n",
    "                # getting embeddings and cross attentions\n",
    "                embedded_per = self.embedding(torch.concat([x_dem_new, x_per_new[v_idx]], dim=0))\n",
    "                embedded_d = self.embedding_d(x_d_new[v_idx])\n",
    "\n",
    "                x_t, s_t = CA(embedded_per, embedded_d, w_dim=self.num_hidden_dim)\n",
    "\n",
    "                h_t_minus_1, ca_t_minus_1, y_t_hat = DSCARNN(h_t_minus_1, ca_t_minus_1, x_t, s_t, self.num_hidden_dim, self.num_codes_d, self.tau).forward()\n",
    "                y_hats = torch.tensor(y_t_hat, dtype=torch.float32).unsqueeze(0)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                # getting embeddings and cross attentions\n",
    "                embedded_per = self.embedding(torch.concat([x_dem_new, x_per_new[v_idx]], dim=0))\n",
    "                embedded_d = self.embedding_d(x_d_new[v_idx])\n",
    "                x_t, s_t = CA(embedded_per, embedded_d, w_dim=self.num_hidden_dim)\n",
    "                \n",
    "                h_t_minus_1, ca_t_minus_1, y_t_hat = DSCARNN(h_t_minus_1, ca_t_minus_1, x_t, s_t, self.num_hidden_dim, self.num_codes_d, self.tau).forward()\n",
    "                y_hats = torch.concat([y_hats, y_t_hat.unsqueeze(0)], dim=0)\n",
    "                \n",
    "\n",
    "\n",
    "        return y_hats\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "dsca_net = DSCANet(num_codes = num_per_codes, num_codes_d=num_codes, tau=0.6)\n",
    "y_hats = dsca_net.forward(x_dem_new, x_per_new, x_d_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSCANet(\n",
       "  (embedding): Linear(in_features=9, out_features=128, bias=True)\n",
       "  (embedding_d): Linear(in_features=400, out_features=128, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (tanh): Tanh()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsca_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([76, 400])"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hats.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parameters() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8h/c4nc3b111l56r38yfxstwq740000gq/T/ipykernel_11756/1358346009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDSCANet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: parameters() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "DSCANet.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8h/c4nc3b111l56r38yfxstwq740000gq/T/ipykernel_11756/2148499787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsca_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(list(dsca_net.parameters(), CA.parameters()), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x_dem_new, x_per_new, x_d_new in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            loss = None\n",
    "            # your code here\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ## model forward\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            \n",
    "            ##loss\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([76, 4])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_per_new.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([409])\n",
      "torch.Size([409])\n"
     ]
    }
   ],
   "source": [
    "print(x_ex.size())\n",
    "print(s_ex.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = x_d_ex.view(10, -1, 128)\n",
    "combsum = torch.sum(comb, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combsum * x_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 47, 128])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_c_ex.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 42, 48, 128])\n",
      "torch.Size([10, 47, 128])\n"
     ]
    }
   ],
   "source": [
    "print(x_d_ex.size())\n",
    "print(x_c_ex.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_d_ex[0][0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "tensor(0.9721, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x_c_ex[0][0].size())\n",
    "print(x_d_ex[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -0.1053,  11.7656,  -0.9722,  -4.5396,   1.8796,  -4.2402,   1.3459,\n",
       "          2.6512,  -2.2932,  -3.7640,   1.5582,  -3.1277,  -1.8824,  -5.7328,\n",
       "          3.5307,   2.6421,  -3.5459,  14.6715,   0.1037,   3.1923,  -4.5923,\n",
       "         -2.3548,   3.7755,  -1.0293,   2.9087,   0.4739,  -5.3497,  -6.9707,\n",
       "         -0.5234,   6.3263,   1.4085,   0.4351,   6.2411,  -6.4672,  -2.6802,\n",
       "         -3.0452,  -2.7206,   1.9852,   4.9574,  -1.2803,   6.8643,   3.8397,\n",
       "         -7.8684,   5.9907,   9.8121,   1.5851,   0.9734,   2.2264,   3.3719,\n",
       "          2.6987,   1.6941,  -3.0267,   5.8485,  -2.0138,  -4.8148,  -9.3796,\n",
       "          1.6449,   7.1585,  -5.9781,   6.6120,   3.1153,   1.9886,   4.7200,\n",
       "         11.3641,   2.7315,   0.1468,  -1.7767,  -8.4672,  -7.4049,  -0.9031,\n",
       "         -6.7645,   1.0623,  -6.6412,   4.6745,   1.0223,  -3.9003,   1.1705,\n",
       "          3.8033,   4.1859,  -7.3584,  -8.8363,   8.7551,   4.4871,   8.2050,\n",
       "         -3.4502,  -5.3145,  -8.8944,   0.0268,  -0.7932,   0.7386,  -3.6330,\n",
       "         -5.4573,  -4.4162,   6.6378,  -2.5632,  -7.1502,  -1.8289,   6.5191,\n",
       "         -3.1980,   2.0723,  -2.4219,   2.3063,  -2.5965,   2.2448,   0.9693,\n",
       "        -14.6585,   3.1395,  -0.9210,   5.0959,   2.8895,  -3.9449,   3.1820,\n",
       "          0.4509,  -7.1079,  -0.1828,  -1.2965,  -7.7691,   9.5871,  -0.4348,\n",
       "         -0.0520,  -1.2091,  -1.0197,  -6.3409,  -0.9624,  -3.5058,   0.3048,\n",
       "         -4.2020,   2.7933], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x_d_ex[0][0][0]) * x_c_ex[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0352,  1.2070,  4.3272,  4.2509,  1.3044,  0.7701, -2.1011, -3.1854,\n",
       "         0.1768, -3.2213, -4.4997,  2.2523,  0.6664, -1.6208, -4.0750, -4.5078,\n",
       "        -0.6047,  4.6722,  0.8967, -1.4821,  7.2332, -5.6826, -0.7302, -5.0149,\n",
       "         0.3665,  3.0565, -1.5576, -1.2695, -2.9509, -2.9607,  8.2866, -2.2148,\n",
       "         4.4720,  4.6280, -3.8665, -3.1833, -3.4565, -0.9128, -1.2805, -0.5828,\n",
       "         1.7183,  2.8481,  2.3334,  2.5972, -0.2080,  0.6289,  0.7385,  2.1967,\n",
       "        -3.3458, -0.9485,  2.9139, -2.2417, -2.0779,  0.6835,  6.1068, -0.7912,\n",
       "         1.9938, -2.1672, -1.1477,  0.9477, -1.8482,  1.4520, -1.0124,  3.1347,\n",
       "         1.2956,  1.2019,  7.7229,  6.2986,  2.1932, -6.4995,  0.7766, -1.5119,\n",
       "         1.7411, -1.5819, -1.3898, -1.6854, -0.5595, -0.2571, -3.3016, -4.5502,\n",
       "        -0.4487,  0.5043, -3.6371, -3.7618, -3.4027, -1.9484, -0.4875, -3.1030,\n",
       "         1.3803, -4.2961,  2.6533, -0.1317, -1.6301,  5.6923, -0.8562, -0.4400,\n",
       "         0.0173, -3.8527,  0.6336, -0.1563, -1.6840, -1.9167,  0.3789, -0.7063,\n",
       "         3.0410,  0.2246,  5.1713,  2.2776, -3.1310,  1.5148, -6.8683, -0.6842,\n",
       "        -2.3905, -0.3891,  0.6157,  1.4722,  4.1876, -2.2358, -7.9065,  0.2541,\n",
       "         1.0258, -0.1732, -3.0206,  3.5158,  0.5369,  2.9112,  2.1982, -1.1004],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x_c_ex[0][0]) * x_d_ex[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 128])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 47, 128 짜리 하나랑. \n",
    "x_d_ex[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 42, 48, 128])\n",
      "torch.Size([10, 47, 128])\n"
     ]
    }
   ],
   "source": [
    "print(x_d_ex.size())\n",
    "print(x_c_ex.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
